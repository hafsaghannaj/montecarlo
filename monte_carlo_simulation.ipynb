{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Portfolio Simulation\n",
    "## Multi-Asset | Regime-Switching | Fat-Tailed | Risk Parity\n",
    "\n",
    "This notebook implements a comprehensive Monte Carlo simulation for a multi-asset portfolio with the following features:\n",
    "\n",
    "- **8 assets** across Equities, Bonds, Commodities, and FX\n",
    "- **Correlated fat-tailed returns** via Cholesky decomposition with Student-t innovations\n",
    "- **3-state Markov regime switching** (Bull / Bear / Crisis) with distinct parameters per regime\n",
    "- **Risk parity allocation** — equal-risk-contribution optimization\n",
    "- **Macro factor decomposition** into Growth, Inflation, and Rate drivers\n",
    "- **Stress testing** against GFC 2008, COVID 2020, Rate Shock, and Stagflation scenarios\n",
    "- **Comprehensive risk metrics**: VaR, CVaR, Sharpe, Sortino, Calmar, Maximum Drawdown\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration\n",
    "\n",
    "We define the full asset universe, simulation parameters, regime-specific return distributions, Markov transition probabilities, macro factor loadings, and stress scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "\n",
    "print(\"Libraries loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Asset Universe ───────────────────────────────────────────────────────────\n",
    "\n",
    "ASSETS = [\n",
    "    \"US_Equity\", \"Intl_Equity\", \"US_Treasury_10Y\", \"TIPS\",\n",
    "    \"Gold\", \"Commodities\", \"USD_EUR\", \"USD_JPY\",\n",
    "]\n",
    "\n",
    "ASSET_CLASSES = {\n",
    "    \"US_Equity\": \"Equity\", \"Intl_Equity\": \"Equity\",\n",
    "    \"US_Treasury_10Y\": \"Bond\", \"TIPS\": \"Bond\",\n",
    "    \"Gold\": \"Commodity\", \"Commodities\": \"Commodity\",\n",
    "    \"USD_EUR\": \"FX\", \"USD_JPY\": \"FX\",\n",
    "}\n",
    "\n",
    "N_ASSETS = len(ASSETS)\n",
    "\n",
    "# ─── Simulation Parameters ────────────────────────────────────────────────────\n",
    "\n",
    "N_SIMULATIONS = 10_000\n",
    "N_YEARS = 10\n",
    "TRADING_DAYS_PER_YEAR = 252\n",
    "N_PERIODS = TRADING_DAYS_PER_YEAR * N_YEARS  # 2520\n",
    "DT = 1.0 / TRADING_DAYS_PER_YEAR\n",
    "RISK_FREE_RATE = 0.04  # annualized\n",
    "SEED = 42\n",
    "\n",
    "print(f\"Asset Universe: {N_ASSETS} assets across {len(set(ASSET_CLASSES.values()))} classes\")\n",
    "print(f\"Simulation: {N_SIMULATIONS:,} paths x {N_PERIODS:,} days ({N_YEARS} years)\")\n",
    "print(f\"Risk-free rate: {RISK_FREE_RATE:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regime Parameters\n",
    "\n",
    "Each market regime has distinct characteristics:\n",
    "\n",
    "| Regime | Expected Returns | Volatility | Correlations | Tail Thickness (df) |\n",
    "|--------|-----------------|------------|--------------|--------------------|\n",
    "| **Bull** | Positive | Low | Moderate | 8 (mild tails) |\n",
    "| **Bear** | Negative/Low | High | Elevated | 5 (moderate tails) |\n",
    "| **Crisis** | Large negative | Very high | Spike toward 1 | 3 (extreme tails) |\n",
    "\n",
    "The correlation spike during crises captures the empirical phenomenon where diversification fails precisely when it's needed most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Regime Parameters ────────────────────────────────────────────────────────\n",
    "\n",
    "# Bull regime — moderate returns, low vol, moderate correlations\n",
    "BULL_MU = np.array([0.10, 0.08, 0.03, 0.04, 0.05, 0.06, 0.01, -0.01])\n",
    "BULL_SIGMA = np.array([0.15, 0.17, 0.06, 0.07, 0.16, 0.20, 0.08, 0.10])\n",
    "BULL_CORR = np.array([\n",
    "    [ 1.00,  0.80, -0.20, -0.10,  0.05,  0.30,  0.10,  0.05],\n",
    "    [ 0.80,  1.00, -0.15, -0.05,  0.10,  0.35,  0.25,  0.15],\n",
    "    [-0.20, -0.15,  1.00,  0.70, -0.05, -0.10, -0.05,  0.10],\n",
    "    [-0.10, -0.05,  0.70,  1.00,  0.15,  0.10,  0.00,  0.05],\n",
    "    [ 0.05,  0.10, -0.05,  0.15,  1.00,  0.40,  0.20,  0.10],\n",
    "    [ 0.30,  0.35, -0.10,  0.10,  0.40,  1.00,  0.15,  0.05],\n",
    "    [ 0.10,  0.25, -0.05,  0.00,  0.20,  0.15,  1.00,  0.30],\n",
    "    [ 0.05,  0.15,  0.10,  0.05,  0.10,  0.05,  0.30,  1.00],\n",
    "])\n",
    "BULL_DF = 8\n",
    "\n",
    "# Bear regime — negative/low returns, high vol, higher correlations\n",
    "BEAR_MU = np.array([-0.05, -0.08, 0.06, 0.03, 0.08, -0.03, 0.02, 0.03])\n",
    "BEAR_SIGMA = np.array([0.25, 0.28, 0.10, 0.12, 0.22, 0.30, 0.12, 0.14])\n",
    "BEAR_CORR = np.array([\n",
    "    [ 1.00,  0.90, -0.30, -0.15,  0.15,  0.50,  0.20,  0.10],\n",
    "    [ 0.90,  1.00, -0.25, -0.10,  0.20,  0.55,  0.35,  0.20],\n",
    "    [-0.30, -0.25,  1.00,  0.75, -0.10, -0.20, -0.10,  0.15],\n",
    "    [-0.15, -0.10,  0.75,  1.00,  0.20,  0.05,  0.00,  0.10],\n",
    "    [ 0.15,  0.20, -0.10,  0.20,  1.00,  0.45,  0.25,  0.15],\n",
    "    [ 0.50,  0.55, -0.20,  0.05,  0.45,  1.00,  0.20,  0.10],\n",
    "    [ 0.20,  0.35, -0.10,  0.00,  0.25,  0.20,  1.00,  0.40],\n",
    "    [ 0.10,  0.20,  0.15,  0.10,  0.15,  0.10,  0.40,  1.00],\n",
    "])\n",
    "BEAR_DF = 5\n",
    "\n",
    "# Crisis regime — large drawdowns, extreme vol, correlations spike\n",
    "CRISIS_MU = np.array([-0.30, -0.35, 0.10, 0.02, 0.15, -0.20, 0.05, 0.08])\n",
    "CRISIS_SIGMA = np.array([0.45, 0.50, 0.15, 0.18, 0.30, 0.45, 0.18, 0.20])\n",
    "CRISIS_CORR = np.array([\n",
    "    [ 1.00,  0.95, -0.40, -0.20,  0.25,  0.70,  0.30,  0.15],\n",
    "    [ 0.95,  1.00, -0.35, -0.15,  0.30,  0.75,  0.45,  0.25],\n",
    "    [-0.40, -0.35,  1.00,  0.80, -0.15, -0.30, -0.15,  0.20],\n",
    "    [-0.20, -0.15,  0.80,  1.00,  0.25,  0.00,  0.00,  0.15],\n",
    "    [ 0.25,  0.30, -0.15,  0.25,  1.00,  0.50,  0.30,  0.20],\n",
    "    [ 0.70,  0.75, -0.30,  0.00,  0.50,  1.00,  0.25,  0.15],\n",
    "    [ 0.30,  0.45, -0.15,  0.00,  0.30,  0.25,  1.00,  0.50],\n",
    "    [ 0.15,  0.25,  0.20,  0.15,  0.20,  0.15,  0.50,  1.00],\n",
    "])\n",
    "CRISIS_DF = 3\n",
    "\n",
    "REGIMES = {\n",
    "    \"bull\":   {\"mu\": BULL_MU,   \"sigma\": BULL_SIGMA,   \"corr\": BULL_CORR,   \"df\": BULL_DF},\n",
    "    \"bear\":   {\"mu\": BEAR_MU,   \"sigma\": BEAR_SIGMA,   \"corr\": BEAR_CORR,   \"df\": BEAR_DF},\n",
    "    \"crisis\": {\"mu\": CRISIS_MU, \"sigma\": CRISIS_SIGMA, \"corr\": CRISIS_CORR, \"df\": CRISIS_DF},\n",
    "}\n",
    "REGIME_NAMES = [\"bull\", \"bear\", \"crisis\"]\n",
    "\n",
    "# ─── Markov Transition Matrix ─────────────────────────────────────────────────\n",
    "TRANSITION_MATRIX = np.array([\n",
    "    [0.980, 0.015, 0.005],  # bull  → bull 98%, bear 1.5%, crisis 0.5%\n",
    "    [0.030, 0.950, 0.020],  # bear  → bull 3%,  bear 95%,  crisis 2%\n",
    "    [0.050, 0.150, 0.800],  # crisis→ bull 5%,  bear 15%,  crisis 80%\n",
    "])\n",
    "\n",
    "print(\"Regime parameters configured.\")\n",
    "print(f\"Transition matrix row sums: {TRANSITION_MATRIX.sum(axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Macro Factor Loadings ────────────────────────────────────────────────────\n",
    "# 8 assets x 3 factors: [Growth, Inflation, Rates]\n",
    "#   Growth:    equities +, bonds -, commodities +\n",
    "#   Inflation: TIPS +, gold +, commodities +, nominal bonds -\n",
    "#   Rates:     bonds -, equities -, TIPS partial hedge\n",
    "\n",
    "FACTOR_LOADINGS = np.array([\n",
    "    [ 0.80,  0.10, -0.20],  # US_Equity\n",
    "    [ 0.75,  0.15, -0.15],  # Intl_Equity\n",
    "    [-0.30, -0.40,  0.70],  # US_Treasury_10Y\n",
    "    [-0.10,  0.50,  0.30],  # TIPS\n",
    "    [ 0.10,  0.60, -0.10],  # Gold\n",
    "    [ 0.40,  0.55, -0.05],  # Commodities\n",
    "    [ 0.20,  0.05, -0.15],  # USD_EUR\n",
    "    [-0.10, -0.05,  0.25],  # USD_JPY\n",
    "])\n",
    "\n",
    "FACTOR_NAMES = [\"Growth\", \"Inflation\", \"Rates\"]\n",
    "\n",
    "# ─── Stress Scenarios ─────────────────────────────────────────────────────────\n",
    "STRESS_SCENARIOS = {\n",
    "    \"GFC 2008\": {\n",
    "        \"US_Equity\": -0.38, \"Intl_Equity\": -0.43, \"US_Treasury_10Y\": 0.20,\n",
    "        \"TIPS\": 0.02, \"Gold\": 0.05, \"Commodities\": -0.36,\n",
    "        \"USD_EUR\": -0.04, \"USD_JPY\": 0.23,\n",
    "    },\n",
    "    \"COVID 2020\": {\n",
    "        \"US_Equity\": -0.34, \"Intl_Equity\": -0.30, \"US_Treasury_10Y\": 0.15,\n",
    "        \"TIPS\": 0.05, \"Gold\": 0.03, \"Commodities\": -0.25,\n",
    "        \"USD_EUR\": -0.02, \"USD_JPY\": 0.03,\n",
    "    },\n",
    "    \"Rate Shock\": {\n",
    "        \"US_Equity\": -0.15, \"Intl_Equity\": -0.12, \"US_Treasury_10Y\": -0.18,\n",
    "        \"TIPS\": -0.08, \"Gold\": -0.05, \"Commodities\": 0.05,\n",
    "        \"USD_EUR\": -0.05, \"USD_JPY\": -0.08,\n",
    "    },\n",
    "    \"Stagflation\": {\n",
    "        \"US_Equity\": -0.20, \"Intl_Equity\": -0.22, \"US_Treasury_10Y\": -0.10,\n",
    "        \"TIPS\": 0.05, \"Gold\": 0.15, \"Commodities\": 0.25,\n",
    "        \"USD_EUR\": 0.03, \"USD_JPY\": -0.05,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"Factor loadings: {FACTOR_LOADINGS.shape[0]} assets x {FACTOR_LOADINGS.shape[1]} factors\")\n",
    "print(f\"Stress scenarios: {list(STRESS_SCENARIOS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Markov Regime-Switching Model\n",
    "\n",
    "The regime path is generated via a discrete-time Markov chain. At each time step, the market transitions between Bull, Bear, and Crisis states according to the transition matrix. The stationary distribution gives us the long-run probability of being in each regime.\n",
    "\n",
    "$$\\pi P = \\pi, \\quad \\sum_i \\pi_i = 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationary_distribution(transition_matrix):\n",
    "    \"\"\"Compute the stationary distribution of a Markov chain.\"\"\"\n",
    "    n = transition_matrix.shape[0]\n",
    "    A = transition_matrix.T - np.eye(n)\n",
    "    A[-1, :] = 1.0\n",
    "    b = np.zeros(n)\n",
    "    b[-1] = 1.0\n",
    "    return np.linalg.solve(A, b)\n",
    "\n",
    "\n",
    "def simulate_regime_paths(transition_matrix, n_sims, n_periods, rng):\n",
    "    \"\"\"\n",
    "    Simulate Markov chain regime paths for all simulations.\n",
    "    Returns array of shape (n_sims, n_periods) with labels 0=bull, 1=bear, 2=crisis.\n",
    "    \"\"\"\n",
    "    n_regimes = transition_matrix.shape[0]\n",
    "    pi = stationary_distribution(transition_matrix)\n",
    "    cum_trans = np.cumsum(transition_matrix, axis=1)\n",
    "\n",
    "    regimes = np.empty((n_sims, n_periods), dtype=np.int32)\n",
    "    regimes[:, 0] = rng.choice(n_regimes, size=n_sims, p=pi)\n",
    "\n",
    "    for t in range(1, n_periods):\n",
    "        u = rng.random(n_sims)\n",
    "        current = regimes[:, t - 1]\n",
    "        cum_probs = cum_trans[current]\n",
    "        regimes[:, t] = (u[:, None] >= cum_probs).sum(axis=1)\n",
    "\n",
    "    return regimes\n",
    "\n",
    "\n",
    "# Show stationary distribution\n",
    "pi = stationary_distribution(TRANSITION_MATRIX)\n",
    "print(\"Stationary Distribution:\")\n",
    "for name, prob in zip(REGIME_NAMES, pi):\n",
    "    print(f\"  {name.capitalize():<8} {prob:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Monte Carlo Simulation Engine\n",
    "\n",
    "The core engine generates correlated, fat-tailed returns using:\n",
    "\n",
    "1. **Independent Student-t draws** — captures fat tails (unlike Gaussian)\n",
    "2. **Variance normalization** — Student-t with $\\nu$ df has variance $\\nu/(\\nu-2)$, so we scale by $\\sqrt{(\\nu-2)/\\nu}$\n",
    "3. **Cholesky decomposition** — induces the correct correlation structure: $r_t = \\mu_{\\Delta t} + L \\cdot z_t$\n",
    "\n",
    "Each regime has pre-computed Cholesky factors, so we avoid recomputing them at every time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_positive_definite(A):\n",
    "    try:\n",
    "        np.linalg.cholesky(A)\n",
    "        return True\n",
    "    except np.linalg.LinAlgError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def nearest_positive_definite(A):\n",
    "    \"\"\"Find the nearest positive-definite matrix (Higham algorithm).\"\"\"\n",
    "    B = (A + A.T) / 2\n",
    "    _, s, V = np.linalg.svd(B)\n",
    "    H = V.T @ np.diag(s) @ V\n",
    "    A2 = (B + H) / 2\n",
    "    A3 = (A2 + A2.T) / 2\n",
    "    if is_positive_definite(A3):\n",
    "        return A3\n",
    "    spacing = np.spacing(np.linalg.norm(A))\n",
    "    I = np.eye(A.shape[0])\n",
    "    k = 1\n",
    "    while not is_positive_definite(A3):\n",
    "        mineig = np.min(np.real(np.linalg.eigvalsh(A3)))\n",
    "        A3 += I * (-mineig * k**2 + spacing)\n",
    "        k += 1\n",
    "    return A3\n",
    "\n",
    "\n",
    "def precompute_regime_params():\n",
    "    \"\"\"Pre-compute Cholesky factors and scaled parameters for each regime.\"\"\"\n",
    "    params = {}\n",
    "    for name in REGIME_NAMES:\n",
    "        r = REGIMES[name]\n",
    "        mu, sigma, corr, df = r[\"mu\"], r[\"sigma\"], r[\"corr\"], r[\"df\"]\n",
    "        cov_annual = np.diag(sigma) @ corr @ np.diag(sigma)\n",
    "        cov_daily = cov_annual * DT\n",
    "        if not is_positive_definite(cov_daily):\n",
    "            cov_daily = nearest_positive_definite(cov_daily)\n",
    "        L = np.linalg.cholesky(cov_daily)\n",
    "        mu_daily = mu * DT\n",
    "        t_scale = np.sqrt((df - 2) / df) if df > 2 else 1.0\n",
    "        params[name] = {\"mu_daily\": mu_daily, \"L\": L, \"df\": df, \"t_scale\": t_scale, \"cov_annual\": cov_annual}\n",
    "    return params\n",
    "\n",
    "\n",
    "def run_monte_carlo(n_sims=N_SIMULATIONS, seed=SEED):\n",
    "    \"\"\"\n",
    "    Run the full Monte Carlo simulation.\n",
    "    Returns dict with 'returns', 'prices', 'regimes' arrays.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    regime_params = precompute_regime_params()\n",
    "    regimes = simulate_regime_paths(TRANSITION_MATRIX, n_sims, N_PERIODS, rng)\n",
    "    returns = np.empty((n_sims, N_PERIODS, N_ASSETS))\n",
    "\n",
    "    for regime_idx, regime_name in enumerate(REGIME_NAMES):\n",
    "        p = regime_params[regime_name]\n",
    "        mask = regimes == regime_idx\n",
    "        n_draws = mask.sum()\n",
    "        if n_draws == 0:\n",
    "            continue\n",
    "        z = stats.t.rvs(df=p[\"df\"], size=(n_draws, N_ASSETS), random_state=rng)\n",
    "        z *= p[\"t_scale\"]\n",
    "        correlated = z @ p[\"L\"].T + p[\"mu_daily\"]\n",
    "        returns[mask] = correlated\n",
    "\n",
    "    prices = np.ones((n_sims, N_PERIODS + 1, N_ASSETS))\n",
    "    prices[:, 1:, :] = np.cumprod(1.0 + returns, axis=1)\n",
    "\n",
    "    return {\"returns\": returns, \"prices\": prices, \"regimes\": regimes}\n",
    "\n",
    "\n",
    "print(\"Simulation engine ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Running {N_SIMULATIONS:,} Monte Carlo simulations...\")\n",
    "t0 = time.time()\n",
    "results = run_monte_carlo(n_sims=N_SIMULATIONS, seed=SEED)\n",
    "t1 = time.time()\n",
    "print(f\"Completed in {t1 - t0:.1f}s\")\n",
    "print(f\"Returns shape: {results['returns'].shape}  (sims x days x assets)\")\n",
    "print(f\"Prices shape:  {results['prices'].shape}\")\n",
    "print(f\"Regimes shape: {results['regimes'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Covariance Estimation & Expected Returns\n",
    "\n",
    "We estimate the covariance matrix and expected returns from the simulated data. A subsample of 500,000 observations is used for covariance estimation to keep computation efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annualized expected returns from simulation\n",
    "mean_daily_returns = results[\"returns\"].mean(axis=(0, 1))\n",
    "mu_annual = mean_daily_returns * TRADING_DAYS_PER_YEAR\n",
    "\n",
    "# Covariance estimation (subsample for efficiency)\n",
    "flat_returns = results[\"returns\"].reshape(-1, N_ASSETS)\n",
    "rng = np.random.default_rng(SEED)\n",
    "subsample_idx = rng.choice(flat_returns.shape[0], size=min(500_000, flat_returns.shape[0]), replace=False)\n",
    "cov_daily = np.cov(flat_returns[subsample_idx], rowvar=False)\n",
    "cov_annual = cov_daily * TRADING_DAYS_PER_YEAR\n",
    "\n",
    "print(\"Expected Annualized Returns:\")\n",
    "for name, ret in zip(ASSETS, mu_annual):\n",
    "    print(f\"  {name:<20} {ret:>8.2%}\")\n",
    "\n",
    "print(f\"\\nAnnualized volatilities:\")\n",
    "for name, vol in zip(ASSETS, np.sqrt(np.diag(cov_annual))):\n",
    "    print(f\"  {name:<20} {vol:>8.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Risk Parity Allocation\n",
    "\n",
    "Risk parity sets portfolio weights such that each asset contributes equally to total portfolio risk. For weights $w$ and covariance matrix $\\Sigma$:\n",
    "\n",
    "$$RC_i = \\frac{w_i \\cdot (\\Sigma w)_i}{\\sqrt{w^T \\Sigma w}}$$\n",
    "\n",
    "We minimize $\\sum_i (RC_i - \\sigma_p/n)^2$ subject to $\\sum w_i = 1$ and $w_i \\geq 0.01$.\n",
    "\n",
    "The initial guess uses inverse-volatility weights, which is already a good approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_contribution(weights, cov):\n",
    "    \"\"\"Compute each asset's percentage contribution to total portfolio risk.\"\"\"\n",
    "    port_var = weights @ cov @ weights\n",
    "    port_vol = np.sqrt(port_var)\n",
    "    marginal = cov @ weights\n",
    "    rc = weights * marginal / port_vol\n",
    "    return rc / rc.sum()\n",
    "\n",
    "\n",
    "def risk_parity_weights(cov):\n",
    "    \"\"\"Compute risk parity weights via optimization.\"\"\"\n",
    "    n = cov.shape[0]\n",
    "    target_rc = np.ones(n) / n\n",
    "\n",
    "    def objective(w):\n",
    "        port_var = w @ cov @ w\n",
    "        port_vol = np.sqrt(port_var)\n",
    "        marginal = cov @ w\n",
    "        rc = w * marginal / port_vol\n",
    "        rc_pct = rc / rc.sum()\n",
    "        return np.sum((rc_pct - target_rc) ** 2)\n",
    "\n",
    "    vols = np.sqrt(np.diag(cov))\n",
    "    w0 = (1.0 / vols) / (1.0 / vols).sum()\n",
    "    constraints = [{\"type\": \"eq\", \"fun\": lambda w: np.sum(w) - 1.0}]\n",
    "    bounds = [(0.01, 0.50)] * n\n",
    "\n",
    "    result = minimize(objective, w0, method=\"SLSQP\", constraints=constraints,\n",
    "                      bounds=bounds, options={\"ftol\": 1e-12, \"maxiter\": 1000})\n",
    "    return result.x if result.success else w0\n",
    "\n",
    "\n",
    "def risk_parity_by_class(weights, cov):\n",
    "    \"\"\"Compute risk contribution grouped by asset class.\"\"\"\n",
    "    rc = risk_contribution(weights, cov)\n",
    "    class_rc = {}\n",
    "    for i, asset in enumerate(ASSETS):\n",
    "        cls = ASSET_CLASSES[asset]\n",
    "        class_rc[cls] = class_rc.get(cls, 0.0) + rc[i]\n",
    "    return class_rc\n",
    "\n",
    "\n",
    "# Optimize\n",
    "weights = risk_parity_weights(cov_annual)\n",
    "risk_contribs = risk_contribution(weights, cov_annual)\n",
    "class_rc = risk_parity_by_class(weights, cov_annual)\n",
    "\n",
    "print(\"Risk Parity Portfolio Allocation:\")\n",
    "print(f\"{'Asset':<20} {'Weight':>10} {'Risk Contrib':>14} {'Class':>10}\")\n",
    "print(\"-\" * 56)\n",
    "for i, asset in enumerate(ASSETS):\n",
    "    print(f\"{asset:<20} {weights[i]:>10.2%} {risk_contribs[i]:>14.2%} {ASSET_CLASSES[asset]:>10}\")\n",
    "\n",
    "print(f\"\\nRisk Contribution by Asset Class:\")\n",
    "for cls, rc_val in class_rc.items():\n",
    "    print(f\"  {cls:<12} {rc_val:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Risk Metrics\n",
    "\n",
    "Comprehensive risk and performance metrics computed across all 10,000 simulations:\n",
    "\n",
    "- **VaR / CVaR**: Value-at-Risk and Conditional VaR (Expected Shortfall)\n",
    "- **Maximum Drawdown**: Largest peak-to-trough decline\n",
    "- **Sharpe Ratio**: $(\\bar{r} - r_f) / \\sigma$\n",
    "- **Sortino Ratio**: Uses downside deviation only\n",
    "- **Calmar Ratio**: Annualized return / max drawdown\n",
    "- **Skewness / Kurtosis**: Distribution shape characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_drawdown(wealth_path):\n",
    "    \"\"\"Compute max drawdown from a 1D wealth path.\"\"\"\n",
    "    peak = np.maximum.accumulate(wealth_path)\n",
    "    drawdown = (peak - wealth_path) / peak\n",
    "    return float(np.max(drawdown))\n",
    "\n",
    "\n",
    "def compute_all_metrics(portfolio_returns, rf=RISK_FREE_RATE):\n",
    "    \"\"\"Compute comprehensive risk/performance metrics.\"\"\"\n",
    "    n_sims, n_periods = portfolio_returns.shape\n",
    "    daily_rf = rf / TRADING_DAYS_PER_YEAR\n",
    "\n",
    "    total_return = np.prod(1.0 + portfolio_returns, axis=1)\n",
    "    n_years = n_periods / TRADING_DAYS_PER_YEAR\n",
    "    annualized_return = total_return ** (1.0 / n_years) - 1.0\n",
    "\n",
    "    daily_vol = np.std(portfolio_returns, axis=1, ddof=1)\n",
    "    annualized_vol = daily_vol * np.sqrt(TRADING_DAYS_PER_YEAR)\n",
    "\n",
    "    sharpe = (annualized_return - rf) / annualized_vol\n",
    "\n",
    "    excess = portfolio_returns - daily_rf\n",
    "    downside = np.where(excess < 0, excess, 0.0)\n",
    "    downside_vol = np.sqrt(np.mean(downside**2, axis=1)) * np.sqrt(TRADING_DAYS_PER_YEAR)\n",
    "    sortino = (annualized_return - rf) / np.where(downside_vol > 0, downside_vol, np.nan)\n",
    "\n",
    "    wealth = np.cumprod(1.0 + portfolio_returns, axis=1)\n",
    "    max_dd = np.array([maximum_drawdown(wealth[i]) for i in range(n_sims)])\n",
    "\n",
    "    calmar = annualized_return / np.where(max_dd > 0, max_dd, np.nan)\n",
    "\n",
    "    all_returns = portfolio_returns.flatten()\n",
    "    var_95 = np.percentile(all_returns, 5)\n",
    "    var_99 = np.percentile(all_returns, 1)\n",
    "    cvar_95 = all_returns[all_returns <= var_95].mean()\n",
    "    cvar_99 = all_returns[all_returns <= var_99].mean()\n",
    "\n",
    "    skew_vals = stats.skew(portfolio_returns, axis=1)\n",
    "    kurt_vals = stats.kurtosis(portfolio_returns, axis=1)\n",
    "\n",
    "    return {\n",
    "        \"annualized_return\": annualized_return, \"annualized_vol\": annualized_vol,\n",
    "        \"sharpe\": sharpe, \"sortino\": sortino, \"max_drawdown\": max_dd, \"calmar\": calmar,\n",
    "        \"skewness\": skew_vals, \"kurtosis\": kurt_vals,\n",
    "        \"var_95\": var_95, \"var_99\": var_99, \"cvar_95\": cvar_95, \"cvar_99\": cvar_99,\n",
    "        \"wealth\": wealth,\n",
    "    }\n",
    "\n",
    "\n",
    "def summarize_metrics(metrics):\n",
    "    \"\"\"Summarize distributions into median [5th, 95th] percentiles.\"\"\"\n",
    "    summary = {}\n",
    "    for key in [\"annualized_return\", \"annualized_vol\", \"sharpe\", \"sortino\",\n",
    "                \"max_drawdown\", \"calmar\", \"skewness\", \"kurtosis\"]:\n",
    "        vals = metrics[key]\n",
    "        vals_clean = vals[np.isfinite(vals)]\n",
    "        if len(vals_clean) > 0:\n",
    "            summary[key] = {\"median\": float(np.median(vals_clean)),\n",
    "                           \"p5\": float(np.percentile(vals_clean, 5)),\n",
    "                           \"p95\": float(np.percentile(vals_clean, 95))}\n",
    "    for key in [\"var_95\", \"var_99\", \"cvar_95\", \"cvar_99\"]:\n",
    "        summary[key] = float(metrics[key])\n",
    "    return summary\n",
    "\n",
    "\n",
    "# Compute portfolio returns and metrics\n",
    "portfolio_returns = np.einsum(\"ijk,k->ij\", results[\"returns\"], weights)\n",
    "metrics = compute_all_metrics(portfolio_returns)\n",
    "summary = summarize_metrics(metrics)\n",
    "\n",
    "# Display\n",
    "def fmt(v, pct=True):\n",
    "    return f\"{v:.2%}\" if pct else f\"{v:.2f}\"\n",
    "\n",
    "print(f\"Portfolio Risk Metrics ({N_SIMULATIONS:,} simulations)\")\n",
    "print(f\"{'Metric':<28} {'Median':>10} {'5th Pct':>10} {'95th Pct':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for key, label, pct in [\n",
    "    (\"annualized_return\", \"Annualized Return\", True),\n",
    "    (\"annualized_vol\", \"Annualized Volatility\", True),\n",
    "    (\"sharpe\", \"Sharpe Ratio\", False),\n",
    "    (\"sortino\", \"Sortino Ratio\", False),\n",
    "    (\"max_drawdown\", \"Maximum Drawdown\", True),\n",
    "    (\"calmar\", \"Calmar Ratio\", False),\n",
    "    (\"skewness\", \"Skewness\", False),\n",
    "    (\"kurtosis\", \"Excess Kurtosis\", False),\n",
    "]:\n",
    "    s = summary[key]\n",
    "    print(f\"{label:<28} {fmt(s['median'], pct):>10} {fmt(s['p5'], pct):>10} {fmt(s['p95'], pct):>10}\")\n",
    "\n",
    "print(f\"\\nDaily VaR (95%):   {summary['var_95']:.4%}\")\n",
    "print(f\"Daily VaR (99%):   {summary['var_99']:.4%}\")\n",
    "print(f\"Daily CVaR (95%):  {summary['cvar_95']:.4%}\")\n",
    "print(f\"Daily CVaR (99%):  {summary['cvar_99']:.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizations\n",
    "\n",
    "### 7.1 Simulation Paths\n",
    "Fan chart showing 100 sample portfolio wealth paths with percentile bands across the 10-year horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color palette\n",
    "COLORS = {\n",
    "    \"bull\": \"#00c853\", \"bear\": \"#ff6d00\", \"crisis\": \"#d50000\",\n",
    "    \"primary\": \"#42a5f5\", \"secondary\": \"#ab47bc\", \"accent\": \"#26c6da\", \"gold\": \"#ffd600\",\n",
    "}\n",
    "ASSET_COLORS = sns.color_palette(\"husl\", len(ASSETS))\n",
    "\n",
    "# Simulation paths\n",
    "wealth = metrics[\"wealth\"]\n",
    "n_sims, n_periods = wealth.shape\n",
    "t = np.arange(n_periods) / TRADING_DAYS_PER_YEAR\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "sample_rng = np.random.default_rng(0)\n",
    "sample_idx = sample_rng.choice(n_sims, size=100, replace=False)\n",
    "for i in sample_idx:\n",
    "    ax.plot(t, wealth[i], alpha=0.05, color=COLORS[\"primary\"], linewidth=0.5)\n",
    "\n",
    "p5 = np.percentile(wealth, 5, axis=0)\n",
    "p25 = np.percentile(wealth, 25, axis=0)\n",
    "p50 = np.median(wealth, axis=0)\n",
    "p75 = np.percentile(wealth, 75, axis=0)\n",
    "p95 = np.percentile(wealth, 95, axis=0)\n",
    "\n",
    "ax.fill_between(t, p5, p95, alpha=0.15, color=COLORS[\"primary\"], label=\"5th-95th pct\")\n",
    "ax.fill_between(t, p25, p75, alpha=0.25, color=COLORS[\"primary\"], label=\"25th-75th pct\")\n",
    "ax.plot(t, p50, color=COLORS[\"gold\"], linewidth=2, label=\"Median\")\n",
    "\n",
    "ax.set_xlabel(\"Years\", fontsize=12)\n",
    "ax.set_ylabel(\"Portfolio Value ($1 initial)\", fontsize=12)\n",
    "ax.set_title(\"Monte Carlo Simulation: 10,000 Portfolio Paths\", fontsize=14, fontweight=\"bold\")\n",
    "ax.legend(loc=\"upper left\", fontsize=10)\n",
    "ax.set_xlim(0, t[-1])\n",
    "ax.grid(alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Return Distribution & Q-Q Plot\n",
    "The histogram shows the distribution of annualized returns with VaR thresholds marked. The Q-Q plot reveals deviation from normality — fat tails from the Student-t innovations are visible at the extremes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ann_ret = metrics[\"annualized_return\"]\n",
    "\n",
    "ax1.hist(ann_ret, bins=80, density=True, alpha=0.7, color=COLORS[\"primary\"], edgecolor=\"none\", label=\"Simulated\")\n",
    "mu_fit, sigma_fit = ann_ret.mean(), ann_ret.std()\n",
    "x = np.linspace(ann_ret.min(), ann_ret.max(), 200)\n",
    "ax1.plot(x, stats.norm.pdf(x, mu_fit, sigma_fit), color=COLORS[\"gold\"],\n",
    "         linewidth=2, label=f\"Normal fit (\\u03bc={mu_fit:.2%}, \\u03c3={sigma_fit:.2%})\")\n",
    "var_5 = np.percentile(ann_ret, 5)\n",
    "var_1 = np.percentile(ann_ret, 1)\n",
    "ax1.axvline(var_5, color=COLORS[\"bear\"], linestyle=\"--\", linewidth=1.5, label=f\"5th pct: {var_5:.2%}\")\n",
    "ax1.axvline(var_1, color=COLORS[\"crisis\"], linestyle=\"--\", linewidth=1.5, label=f\"1st pct: {var_1:.2%}\")\n",
    "ax1.set_xlabel(\"Annualized Return\", fontsize=11)\n",
    "ax1.set_ylabel(\"Density\", fontsize=11)\n",
    "ax1.set_title(\"Distribution of Annualized Returns\", fontsize=13, fontweight=\"bold\")\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.xaxis.set_major_formatter(mticker.PercentFormatter(1.0))\n",
    "ax1.grid(alpha=0.2)\n",
    "\n",
    "stats.probplot(ann_ret, dist=\"norm\", plot=ax2)\n",
    "ax2.set_title(\"Q-Q Plot vs. Normal\", fontsize=13, fontweight=\"bold\")\n",
    "ax2.get_lines()[0].set_color(COLORS[\"primary\"])\n",
    "ax2.get_lines()[0].set_markersize(2)\n",
    "ax2.get_lines()[1].set_color(COLORS[\"crisis\"])\n",
    "ax2.grid(alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Regime Distribution Over Time\n",
    "Shows the fraction of simulations in each market regime at every point in time. The Markov chain converges to its stationary distribution, demonstrating ergodicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regimes = results[\"regimes\"]\n",
    "n_sims_r, n_periods_r = regimes.shape\n",
    "t_r = np.arange(n_periods_r) / TRADING_DAYS_PER_YEAR\n",
    "\n",
    "fractions = np.zeros((3, n_periods_r))\n",
    "for r in range(3):\n",
    "    fractions[r] = (regimes == r).mean(axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "colors = [COLORS[\"bull\"], COLORS[\"bear\"], COLORS[\"crisis\"]]\n",
    "labels = [\"Bull\", \"Bear\", \"Crisis\"]\n",
    "ax.stackplot(t_r, fractions, colors=colors, labels=labels, alpha=0.85)\n",
    "ax.set_xlabel(\"Years\", fontsize=12)\n",
    "ax.set_ylabel(\"Fraction of Simulations\", fontsize=12)\n",
    "ax.set_title(\"Regime Distribution Over Time (Markov Chain)\", fontsize=14, fontweight=\"bold\")\n",
    "ax.legend(loc=\"upper right\", fontsize=10)\n",
    "ax.set_xlim(0, t_r[-1])\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Risk Parity Allocation\n",
    "Capital weights (left) vs. risk contribution (right). Despite unequal capital allocation, each asset contributes exactly 12.5% of total portfolio risk — the defining property of risk parity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "x = np.arange(len(ASSETS))\n",
    "short_names = [a.replace(\"_\", \"\\n\") for a in ASSETS]\n",
    "\n",
    "ax1.bar(x, weights, color=ASSET_COLORS, edgecolor=\"white\", linewidth=0.5)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(short_names, fontsize=8)\n",
    "ax1.set_ylabel(\"Weight\", fontsize=11)\n",
    "ax1.set_title(\"Capital Allocation\", fontsize=13, fontweight=\"bold\")\n",
    "ax1.yaxis.set_major_formatter(mticker.PercentFormatter(1.0))\n",
    "ax1.grid(alpha=0.2, axis=\"y\")\n",
    "\n",
    "ax2.bar(x, risk_contribs, color=ASSET_COLORS, edgecolor=\"white\", linewidth=0.5)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(short_names, fontsize=8)\n",
    "ax2.set_ylabel(\"Risk Contribution\", fontsize=11)\n",
    "ax2.set_title(\"Risk Contribution (Target: Equal)\", fontsize=13, fontweight=\"bold\")\n",
    "ax2.yaxis.set_major_formatter(mticker.PercentFormatter(1.0))\n",
    "ax2.axhline(1.0 / len(ASSETS), color=COLORS[\"gold\"], linestyle=\"--\",\n",
    "            linewidth=1.5, label=f\"Target: {1/len(ASSETS):.1%}\")\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(alpha=0.2, axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Efficient Frontier\n",
    "The classical Markowitz efficient frontier with individual assets, the risk parity portfolio, and the equal-weight portfolio plotted for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_frontier(mu, cov, n_points=50):\n",
    "    \"\"\"Compute the efficient frontier via quadratic optimization.\"\"\"\n",
    "    n = len(mu)\n",
    "    def portfolio_vol(w):\n",
    "        return np.sqrt(w @ cov @ w)\n",
    "\n",
    "    min_ret, max_ret = mu.min(), mu.max()\n",
    "    target_returns = np.linspace(min_ret, max_ret, n_points)\n",
    "    frontier_vols, frontier_rets, frontier_weights = [], [], []\n",
    "    w0 = np.ones(n) / n\n",
    "    bounds = [(0.0, 1.0)] * n\n",
    "\n",
    "    for target in target_returns:\n",
    "        constraints = [\n",
    "            {\"type\": \"eq\", \"fun\": lambda w: np.sum(w) - 1.0},\n",
    "            {\"type\": \"eq\", \"fun\": lambda w, t=target: w @ mu - t},\n",
    "        ]\n",
    "        result = minimize(portfolio_vol, w0, method=\"SLSQP\", constraints=constraints,\n",
    "                          bounds=bounds, options={\"ftol\": 1e-10, \"maxiter\": 500})\n",
    "        if result.success:\n",
    "            frontier_vols.append(portfolio_vol(result.x))\n",
    "            frontier_rets.append(target)\n",
    "            frontier_weights.append(result.x.copy())\n",
    "\n",
    "    return {\"returns\": np.array(frontier_rets), \"vols\": np.array(frontier_vols),\n",
    "            \"weights\": np.array(frontier_weights)}\n",
    "\n",
    "frontier = efficient_frontier(mu_annual, cov_annual, n_points=50)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.plot(frontier[\"vols\"], frontier[\"returns\"], color=COLORS[\"primary\"], linewidth=2, label=\"Efficient Frontier\")\n",
    "\n",
    "asset_vols = np.sqrt(np.diag(cov_annual))\n",
    "for i, (name, vol, ret) in enumerate(zip(ASSETS, asset_vols, mu_annual)):\n",
    "    ax.scatter(vol, ret, color=ASSET_COLORS[i], s=80, zorder=5, edgecolors=\"white\")\n",
    "    ax.annotate(name.replace(\"_\", \" \"), (vol, ret), textcoords=\"offset points\",\n",
    "                xytext=(8, 4), fontsize=8, color=\"white\")\n",
    "\n",
    "rp_vol = np.sqrt(weights @ cov_annual @ weights)\n",
    "rp_ret = weights @ mu_annual\n",
    "ax.scatter(rp_vol, rp_ret, color=COLORS[\"gold\"], s=200, marker=\"*\",\n",
    "           zorder=10, edgecolors=\"white\", linewidth=1, label=\"Risk Parity\")\n",
    "\n",
    "ew = np.ones(len(mu_annual)) / len(mu_annual)\n",
    "ew_vol = np.sqrt(ew @ cov_annual @ ew)\n",
    "ew_ret = ew @ mu_annual\n",
    "ax.scatter(ew_vol, ew_ret, color=COLORS[\"accent\"], s=100, marker=\"D\",\n",
    "           zorder=10, edgecolors=\"white\", linewidth=1, label=\"Equal Weight\")\n",
    "\n",
    "ax.set_xlabel(\"Annualized Volatility\", fontsize=12)\n",
    "ax.set_ylabel(\"Annualized Return\", fontsize=12)\n",
    "ax.set_title(\"Efficient Frontier with Risk Parity Portfolio\", fontsize=14, fontweight=\"bold\")\n",
    "ax.xaxis.set_major_formatter(mticker.PercentFormatter(1.0))\n",
    "ax.yaxis.set_major_formatter(mticker.PercentFormatter(1.0))\n",
    "ax.legend(fontsize=10, loc=\"upper left\")\n",
    "ax.grid(alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Macro Factor Decomposition\n",
    "\n",
    "We decompose each asset's return variance into contributions from three systematic macro factors:\n",
    "- **Growth**: Captures economic expansion/contraction sensitivity\n",
    "- **Inflation**: Captures real asset vs. nominal asset dynamics  \n",
    "- **Rates**: Captures interest rate sensitivity (duration)\n",
    "\n",
    "The decomposition uses OLS projection: $F = (B^T B)^{-1} B^T R^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_returns(returns):\n",
    "    \"\"\"Decompose asset returns into macro factor components via OLS projection.\"\"\"\n",
    "    B = FACTOR_LOADINGS\n",
    "    R = returns.mean(axis=0) if returns.ndim == 3 else returns\n",
    "\n",
    "    BtB_inv = np.linalg.inv(B.T @ B)\n",
    "    factor_returns = (BtB_inv @ B.T @ R.T).T\n",
    "    explained = factor_returns @ B.T\n",
    "    residual = R - explained\n",
    "\n",
    "    n_assets, n_factors = B.shape\n",
    "    var_decomp = np.zeros((n_assets, n_factors + 1))\n",
    "    for f in range(n_factors):\n",
    "        factor_component = np.outer(factor_returns[:, f], B[:, f])\n",
    "        var_decomp[:, f] = np.var(factor_component, axis=0)\n",
    "    var_decomp[:, -1] = np.var(residual, axis=0)\n",
    "\n",
    "    row_sums = var_decomp.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0] = 1.0\n",
    "    return var_decomp / row_sums\n",
    "\n",
    "\n",
    "var_decomp = decompose_returns(results[\"returns\"])\n",
    "\n",
    "# Plot\n",
    "factor_labels = [\"Growth\", \"Inflation\", \"Rates\", \"Idiosyncratic\"]\n",
    "factor_colors = [COLORS[\"primary\"], COLORS[\"bear\"], COLORS[\"accent\"], \"#757575\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "x = np.arange(len(ASSETS))\n",
    "short_names = [a.replace(\"_\", \"\\n\") for a in ASSETS]\n",
    "\n",
    "bottom = np.zeros(len(ASSETS))\n",
    "for f in range(4):\n",
    "    ax.bar(x, var_decomp[:, f], bottom=bottom, color=factor_colors[f],\n",
    "           label=factor_labels[f], edgecolor=\"white\", linewidth=0.5)\n",
    "    bottom += var_decomp[:, f]\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(short_names, fontsize=9)\n",
    "ax.set_ylabel(\"Fraction of Variance\", fontsize=11)\n",
    "ax.set_title(\"Macro Factor Variance Decomposition\", fontsize=14, fontweight=\"bold\")\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.yaxis.set_major_formatter(mticker.PercentFormatter(1.0))\n",
    "ax.grid(alpha=0.2, axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Stress Testing\n",
    "\n",
    "We apply four historical/hypothetical stress scenarios to the risk parity portfolio. Each scenario defines asset-level shocks calibrated to observed market behavior during:\n",
    "\n",
    "| Scenario | Period | Key Driver |\n",
    "|----------|--------|------------|\n",
    "| **GFC 2008** | Sep 2008 - Mar 2009 | Credit crisis, equity crash, flight to quality |\n",
    "| **COVID 2020** | Feb - Mar 2020 | Pandemic shock, indiscriminate selling |\n",
    "| **Rate Shock** | Hypothetical | Rapid rate hikes, bond/equity correlation flip |\n",
    "| **Stagflation** | Hypothetical | Rising inflation + falling growth |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stress_tests(weights, scenarios=STRESS_SCENARIOS):\n",
    "    \"\"\"Run all stress scenarios against the portfolio.\"\"\"\n",
    "    rows = []\n",
    "    for name, shocks in scenarios.items():\n",
    "        shock_vec = np.array([shocks[a] for a in ASSETS])\n",
    "        portfolio_impact = weights @ shock_vec\n",
    "        contributions = weights * shock_vec\n",
    "        row = {\"Scenario\": name, \"Portfolio_Impact\": portfolio_impact}\n",
    "        for i, asset in enumerate(ASSETS):\n",
    "            row[asset] = contributions[i]\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows).set_index(\"Scenario\")\n",
    "\n",
    "\n",
    "stress_df = run_stress_tests(weights)\n",
    "\n",
    "# Display table\n",
    "print(\"Stress Test Results:\")\n",
    "print(f\"{'Scenario':<20} {'Portfolio Impact':>18}\")\n",
    "print(\"-\" * 40)\n",
    "for scenario in stress_df.index:\n",
    "    impact = stress_df.loc[scenario, \"Portfolio_Impact\"]\n",
    "    print(f\"{scenario:<20} {impact:>18.2%}\")\n",
    "\n",
    "# Waterfall chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "scenarios = stress_df.index.tolist()\n",
    "impacts = stress_df[\"Portfolio_Impact\"].values\n",
    "bar_colors = [COLORS[\"crisis\"] if v < 0 else COLORS[\"bull\"] for v in impacts]\n",
    "\n",
    "bars = ax.barh(scenarios, impacts, color=bar_colors, edgecolor=\"white\", height=0.5)\n",
    "for bar, val in zip(bars, impacts):\n",
    "    x_pos = val - 0.005 if val < 0 else val + 0.005\n",
    "    ha = \"right\" if val < 0 else \"left\"\n",
    "    ax.text(x_pos, bar.get_y() + bar.get_height() / 2,\n",
    "            f\"{val:.1%}\", ha=ha, va=\"center\", fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "ax.axvline(0, color=\"white\", linewidth=0.5)\n",
    "ax.set_xlabel(\"Portfolio Impact\", fontsize=12)\n",
    "ax.set_title(\"Stress Test Results\", fontsize=14, fontweight=\"bold\")\n",
    "ax.xaxis.set_major_formatter(mticker.PercentFormatter(1.0))\n",
    "ax.grid(alpha=0.2, axis=\"x\")\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Drawdown Distribution\n",
    "Distribution of maximum drawdowns across all 10,000 simulated paths over the 10-year horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "max_dd = metrics[\"max_drawdown\"]\n",
    "ax.hist(max_dd, bins=80, density=True, alpha=0.7, color=COLORS[\"crisis\"], edgecolor=\"none\")\n",
    "\n",
    "p50_dd = np.median(max_dd)\n",
    "p95_dd = np.percentile(max_dd, 95)\n",
    "p99_dd = np.percentile(max_dd, 99)\n",
    "\n",
    "ax.axvline(p50_dd, color=COLORS[\"gold\"], linestyle=\"--\", linewidth=1.5, label=f\"Median: {p50_dd:.1%}\")\n",
    "ax.axvline(p95_dd, color=COLORS[\"bear\"], linestyle=\"--\", linewidth=1.5, label=f\"95th pct: {p95_dd:.1%}\")\n",
    "ax.axvline(p99_dd, color=\"white\", linestyle=\"--\", linewidth=1.5, label=f\"99th pct: {p99_dd:.1%}\")\n",
    "\n",
    "ax.set_xlabel(\"Maximum Drawdown\", fontsize=12)\n",
    "ax.set_ylabel(\"Density\", fontsize=12)\n",
    "ax.set_title(\"Distribution of Maximum Drawdowns (10-Year Horizon)\", fontsize=14, fontweight=\"bold\")\n",
    "ax.xaxis.set_major_formatter(mticker.PercentFormatter(1.0))\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Risk parity achieves perfect equal risk contribution** — each of the 8 assets contributes exactly 12.5% of portfolio risk, and each asset class contributes 25%\n",
    "\n",
    "2. **Fat tails are present** — excess kurtosis significantly above 0 confirms that the Student-t innovations produce realistic heavy-tailed return distributions\n",
    "\n",
    "3. **Regime switching creates path-dependent dynamics** — crisis regimes simultaneously increase volatility, spike correlations (destroying diversification), and produce negative expected returns\n",
    "\n",
    "4. **The portfolio is resilient to equity-driven shocks** (GFC, COVID) thanks to bond and gold hedges, but vulnerable to rate shocks that hit bonds and equities simultaneously\n",
    "\n",
    "5. **Risk parity sits near the minimum-variance point** on the efficient frontier, prioritizing risk management over return maximization\n",
    "\n",
    "### Methodology\n",
    "\n",
    "| Component | Approach |\n",
    "|-----------|----------|\n",
    "| Return generation | Cholesky decomposition + Student-t innovations |\n",
    "| Regime dynamics | 3-state Markov chain (Bull/Bear/Crisis) |\n",
    "| Allocation | Risk parity via SLSQP optimization |\n",
    "| Factor model | OLS projection onto Growth/Inflation/Rates |\n",
    "| Risk metrics | Historical VaR/CVaR, drawdown, Sharpe/Sortino/Calmar |\n",
    "| Stress testing | Deterministic scenario shocks |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final metrics table\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.axis(\"off\")\n",
    "\n",
    "rows = []\n",
    "row_labels = []\n",
    "\n",
    "for key, label, pct in [\n",
    "    (\"annualized_return\", \"Annualized Return\", True),\n",
    "    (\"annualized_vol\", \"Annualized Volatility\", True),\n",
    "    (\"sharpe\", \"Sharpe Ratio\", False),\n",
    "    (\"sortino\", \"Sortino Ratio\", False),\n",
    "    (\"max_drawdown\", \"Maximum Drawdown\", True),\n",
    "    (\"calmar\", \"Calmar Ratio\", False),\n",
    "    (\"skewness\", \"Skewness\", False),\n",
    "    (\"kurtosis\", \"Excess Kurtosis\", False),\n",
    "]:\n",
    "    s = summary[key]\n",
    "    rows.append([fmt(s[\"median\"], pct), fmt(s[\"p5\"], pct), fmt(s[\"p95\"], pct)])\n",
    "    row_labels.append(label)\n",
    "\n",
    "for key, label in [(\"var_95\", \"Daily VaR (95%)\"), (\"var_99\", \"Daily VaR (99%)\"),\n",
    "                   (\"cvar_95\", \"Daily CVaR (95%)\"), (\"cvar_99\", \"Daily CVaR (99%)\")]:\n",
    "    rows.append([f\"{summary[key]:.4%}\", \"\\u2014\", \"\\u2014\"])\n",
    "    row_labels.append(label)\n",
    "\n",
    "col_labels = [\"Median\", \"5th Pct\", \"95th Pct\"]\n",
    "table = ax.table(cellText=rows, rowLabels=row_labels, colLabels=col_labels,\n",
    "                 cellLoc=\"center\", loc=\"center\")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.6)\n",
    "\n",
    "for (row, col), cell in table.get_celld().items():\n",
    "    cell.set_edgecolor(\"#555555\")\n",
    "    if row == 0:\n",
    "        cell.set_facecolor(\"#333333\")\n",
    "        cell.set_text_props(fontweight=\"bold\", color=\"white\")\n",
    "    elif col == -1:\n",
    "        cell.set_facecolor(\"#2a2a2a\")\n",
    "        cell.set_text_props(fontweight=\"bold\", color=\"white\")\n",
    "    else:\n",
    "        cell.set_facecolor(\"#1a1a1a\")\n",
    "        cell.set_text_props(color=\"white\")\n",
    "\n",
    "ax.set_title(\"Portfolio Risk Metrics Summary (10,000 Simulations)\",\n",
    "             fontsize=14, fontweight=\"bold\", pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
